"""
ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ç•°ãªã‚‹ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆFAISS, ChromaDBç­‰ï¼‰ã«å¯¾ã—ã¦çµ±ä¸€çš„ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚
åŒæœŸå‡¦ç†ã¨éåŒæœŸå‡¦ç†ã®ä¸¡æ–¹ã«å¯¾å¿œã—ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ã¨ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ã‚’çµ„ã¿è¾¼ã‚“ã§ã„ã¾ã™ã€‚
"""

import abc
import os
import json
import pickle
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, TypeVar, Tuple
import hashlib

from app.config import settings
from app.exceptions import CaseforgeException, ErrorCode
from app.utils.path_manager import path_manager
from app.logging_config import logger
from app.utils.retry import retry, async_retry, RetryStrategy
from app.utils.timeout import timeout, async_timeout

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS, Chroma
from faiss import IndexFlatL2
from langchain_community.docstore.in_memory import InMemoryDocstore

from app.services.vector_db.embeddings import (
    EmbeddingModel,
    EmbeddingModelFactory,
    EmbeddingModelWrapper,
    EmbeddingException
)

T = TypeVar('T')
VectorDBManagerType = TypeVar('VectorDBManagerType', bound='VectorDBManager')


class VectorDBException(CaseforgeException):
    """ãƒ™ã‚¯ãƒˆãƒ«DBé–¢é€£ã®ä¾‹å¤–"""
    def __init__(
        self,
        message: str = "ãƒ™ã‚¯ãƒˆãƒ«DBæ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
        details: Optional[Dict[str, Any]] = None
    ):
        super().__init__(message, ErrorCode.VECTOR_DB_ERROR, details)


class DocumentCache:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, cache_dir: str = None, ttl: int = 3600):
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åˆæœŸåŒ–
        
        Args:
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            ttl: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æœŸé–“ï¼ˆç§’ï¼‰
        """
        self.cache_dir = cache_dir or path_manager.join_path(
            os.environ.get("DATA_DIR", "/app/data"),
            "document_cache"
        )
        self.ttl = ttl
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        path_manager.ensure_dir(self.cache_dir)
    
    def _get_cache_key(self, key: str) -> str:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’å–å¾—
        
        Args:
            key: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
            
        Returns:
            ãƒãƒƒã‚·ãƒ¥åŒ–ã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        """
        return hashlib.md5(key.encode()).hexdigest()
    
    def _get_cache_path(self, key: str) -> str:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
        
        Args:
            key: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
            
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        cache_key = self._get_cache_key(key)
        return path_manager.join_path(self.cache_dir, f"{cache_key}.pkl")
    
    def get(self, key: str) -> Optional[List[Document]]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        
        Args:
            key: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
            
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€å­˜åœ¨ã—ãªã„å ´åˆã¯None
        """
        cache_path = self._get_cache_path(key)
        
        if not path_manager.exists(cache_path):
            return None
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æœŸé™ã‚’ãƒã‚§ãƒƒã‚¯
        mtime = os.path.getmtime(str(cache_path))
        if datetime.fromtimestamp(mtime) + timedelta(seconds=self.ttl) < datetime.now():
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æœŸé™åˆ‡ã‚Œ
            os.remove(cache_path)
            return None
        
        try:
            with open(cache_path, "rb") as f:
                return pickle.load(f)
        except Exception as e:
            logger.error(f"Error loading document cache: {e}", exc_info=True)
            # ç ´æã—ãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
            os.remove(cache_path)
            return None
    
    def set(self, key: str, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        
        Args:
            key: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
            documents: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        cache_path = self._get_cache_path(key)
        
        try:
            with open(cache_path, "wb") as f:
                pickle.dump(documents, f)
        except Exception as e:
            logger.error(f"Error saving document cache: {e}", exc_info=True)
            # ä¿å­˜ã«å¤±æ•—ã—ãŸå ´åˆã€éƒ¨åˆ†çš„ã«æ›¸ãè¾¼ã¾ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if path_manager.exists(cache_path):
                os.remove(str(cache_path))
    
    def clear(self, key: Optional[str] = None) -> None:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        
        Args:
            key: ã‚¯ãƒªã‚¢ã™ã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã€Noneã®å ´åˆã¯å…¨ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        """
        if key is None:
            # å…¨ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
            for file in os.listdir(str(self.cache_dir)):
                if file.endswith(".pkl"):
                    os.remove(path_manager.join_path(self.cache_dir, file))
        else:
            # ç‰¹å®šã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
            cache_path = self._get_cache_path(key)
            if path_manager.exists(cache_path):
                os.remove(str(cache_path))
    
    def cleanup_expired(self) -> int:
        """
        æœŸé™åˆ‡ã‚Œã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        
        Returns:
            å‰Šé™¤ã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ•°
        """
        count = 0


class VectorDBManager(abc.ABC):
    """ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        embedding_model: Optional[EmbeddingModel] = None,
        persist_directory: Optional[str] = None,
        collection_name: Optional[str] = None,
        timeout_seconds: Optional[float] = None,
        retry_config: Optional[Dict[str, Any]] = None,
        cache_config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """
        ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
        
        Args:
            embedding_model: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
            persist_directory: æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
            timeout_seconds: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
            retry_config: ãƒªãƒˆãƒ©ã‚¤è¨­å®š
            cache_config: ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
        self.embedding_model = embedding_model or EmbeddingModelFactory.create_default()
        self.embedding_function = EmbeddingModelWrapper(self.embedding_model)
        
        # æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        self.persist_directory = persist_directory
        if self.persist_directory:
            os.makedirs(self.persist_directory, exist_ok=True)
        
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã®è¨­å®š
        self.collection_name = collection_name or "default"
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ã®è¨­å®š
        self.timeout_seconds = timeout_seconds or settings.TIMEOUT_EMBEDDING
        self.retry_config = retry_config or {
            "max_retries": 3,
            "retry_delay": 2.0,
            "max_retry_delay": 10.0,
            "retry_jitter": 0.1,
            "backoff_factor": 2.0,
            "retry_strategy": RetryStrategy.EXPONENTIAL,
            "retry_exceptions": [ConnectionError, TimeoutError, ValueError]
        }
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š
        cache_config = cache_config or {}
        self.document_cache = DocumentCache(
            cache_dir=cache_config.get("cache_dir"),
            ttl=cache_config.get("ttl", 3600)
        )
        self.use_cache = cache_config.get("use_cache", True)
        
        # ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.extra_params = kwargs
        
        # ãƒ™ã‚¯ãƒˆãƒ«DBã®åˆæœŸåŒ–
        self.vectordb = None
        self._setup_vectordb()
    
    @abc.abstractmethod
    def _setup_vectordb(self) -> None:
        """ãƒ™ã‚¯ãƒˆãƒ«DBã®è¨­å®šï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        pass
    
    @abc.abstractmethod
    def _add_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ ã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    def _similarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    def _similarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    def _save(self) -> None:
        """
        ãƒ™ã‚¯ãƒˆãƒ«DBã‚’ä¿å­˜ã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        """
        pass
    
    @abc.abstractmethod
    async def _aadd_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«éåŒæœŸã§è¿½åŠ ã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    async def _asimilarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    async def _asimilarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    async def _asave(self) -> None:
        """
        ãƒ™ã‚¯ãƒˆãƒ«DBã‚’éåŒæœŸã§ä¿å­˜ã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        """
        pass
    
    def _get_cache_key_for_documents(self, documents: List[Document]) -> str:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            documents: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        """
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
        content_hash = hashlib.md5()
        for doc in documents:
            content_hash.update(doc.page_content.encode())
            if doc.metadata:
                content_hash.update(json.dumps(doc.metadata, sort_keys=True).encode())
        
        return f"docs_{content_hash.hexdigest()}"
    
    def _get_cache_key_for_query(self, query: str, k: int, filter: Optional[Dict[str, Any]]) -> str:
        """
        ã‚¯ã‚¨ãƒªã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        """
        # ã‚¯ã‚¨ãƒªã¨ãƒ•ã‚£ãƒ«ã‚¿ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
        query_hash = hashlib.md5()
        query_hash.update(query.encode())
        query_hash.update(str(k).encode())
        if filter:
            query_hash.update(json.dumps(filter, sort_keys=True).encode())
        
        return f"query_{query_hash.hexdigest()}"
    
    @retry(retry_key="VECTOR_DB")
    @timeout(timeout_key="VECTOR_DB")
    def add_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ ã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        try:
            logger.info(f"Adding {len(documents)} documents to vector database")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¿½åŠ ã•ã‚Œã‚‹ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åŠ¹ã«ãªã‚‹ãŸã‚ï¼‰
            if self.use_cache:
                self.document_cache.clear()
            
            self._add_documents(documents)
            
            # å¤‰æ›´ã‚’ä¿å­˜
            if self.persist_directory:
                self._save()
            
            logger.info(f"Successfully added {len(documents)} documents to vector database")
        except Exception as e:
            logger.error(f"Error adding documents to vector database: {e}", exc_info=True)
            raise VectorDBException(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "error": str(e)
            })
    
    @retry(retry_key="VECTOR_DB")
    @timeout(timeout_key="VECTOR_DB")
    def similarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        try:
            logger.info(f"Performing similarity search for query: {query[:30]}...")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            if self.use_cache:
                cache_key = self._get_cache_key_for_query(query, k, filter)
                cached_results = self.document_cache.get(cache_key)
                if cached_results:
                    logger.info(f"Using cached results for query: {query[:30]}...")
                    return cached_results
            
            results = self._similarity_search(query, k, filter)
            
            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            if self.use_cache:
                self.document_cache.set(cache_key, results)
            
            logger.info(f"Successfully performed similarity search, found {len(results)} documents")
            return results
        except Exception as e:
            logger.error(f"Error performing similarity search: {e}", exc_info=True)
            raise VectorDBException(f"é¡ä¼¼åº¦æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "query": query,
                "error": str(e)
            })
    
    @retry(retry_key="VECTOR_DB")
    @timeout(timeout_key="VECTOR_DB")
    def similarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        try:
            logger.info(f"Performing similarity search with score for query: {query[:30]}...")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä½¿ç”¨ã—ãªã„ï¼ˆã‚¹ã‚³ã‚¢ãŒå«ã¾ã‚Œã‚‹ãŸã‚ï¼‰
            
            results = self._similarity_search_with_score(query, k, filter)
            
            logger.info(f"Successfully performed similarity search with score, found {len(results)} documents")
            return results
        except Exception as e:
            logger.error(f"Error performing similarity search with score: {e}", exc_info=True)
            raise VectorDBException(f"ã‚¹ã‚³ã‚¢ä»˜ãé¡ä¼¼åº¦æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "query": query,
                "error": str(e)
            })
    
    @async_retry(retry_key="VECTOR_DB")
    @async_timeout(timeout_key="VECTOR_DB")
    async def aadd_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«éåŒæœŸã§è¿½åŠ ã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        try:
            logger.info(f"Async adding {len(documents)} documents to vector database")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¿½åŠ ã•ã‚Œã‚‹ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åŠ¹ã«ãªã‚‹ãŸã‚ï¼‰
            if self.use_cache:
                self.document_cache.clear()
            
            await self._aadd_documents(documents)
            
            # å¤‰æ›´ã‚’ä¿å­˜
            if self.persist_directory:
                await self._asave()
            
            logger.info(f"Successfully async added {len(documents)} documents to vector database")
        except Exception as e:
            logger.error(f"Error async adding documents to vector database: {e}", exc_info=True)
            raise VectorDBException(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆéåŒæœŸè¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "error": str(e)
            })
    
    @async_retry(retry_key="VECTOR_DB")
    @async_timeout(timeout_key="VECTOR_DB")
    async def asimilarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        try:
            logger.info(f"Async performing similarity search for query: {query[:30]}...")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            if self.use_cache:
                cache_key = self._get_cache_key_for_query(query, k, filter)
                cached_results = self.document_cache.get(cache_key)
                if cached_results:
                    logger.info(f"Using cached results for query: {query[:30]}...")
                    return cached_results
            
            results = await self._asimilarity_search(query, k, filter)
            
            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            if self.use_cache:
                self.document_cache.set(cache_key, results)
            
            logger.info(f"Successfully async performed similarity search, found {len(results)} documents")
            return results
        except Exception as e:
            logger.error(f"Error async performing similarity search: {e}", exc_info=True)
            raise VectorDBException(f"éåŒæœŸé¡ä¼¼åº¦æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "query": query,
                "error": str(e)
            })
    
    @async_retry(retry_key="VECTOR_DB")
    @async_timeout(timeout_key="VECTOR_DB")
    async def asimilarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        try:
            logger.info(f"Async performing similarity search with score for query: {query[:30]}...")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä½¿ç”¨ã—ãªã„ï¼ˆã‚¹ã‚³ã‚¢ãŒå«ã¾ã‚Œã‚‹ãŸã‚ï¼‰
            
            results = await self._asimilarity_search_with_score(query, k, filter)
            
            logger.info(f"Successfully async performed similarity search with score, found {len(results)} documents")
            return results
        except Exception as e:
            logger.error(f"Error async performing similarity search with score: {e}", exc_info=True)
            raise VectorDBException(f"éåŒæœŸã‚¹ã‚³ã‚¢ä»˜ãé¡ä¼¼åº¦æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "query": query,
                "error": str(e)
            })
    
    def clear_cache(self) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        if self.use_cache:
            self.document_cache.clear()
            logger.info("Document cache cleared")
        for file in os.listdir(str(self.cache_dir)):
            if file.endswith(".pkl"):
                file_path = path_manager.join_path(self.cache_dir, file)
                mtime = os.path.getmtime(str(file_path))
                if datetime.fromtimestamp(mtime) + timedelta(seconds=self.ttl) < datetime.now():
                    os.remove(str(file_path))
                    count += 1
        return count


class FAISSManager(VectorDBManager):
    """FAISSãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    
    def _setup_vectordb(self) -> None:
        """FAISSãƒ™ã‚¯ãƒˆãƒ«DBã®è¨­å®š"""
        try:
            if self.persist_directory and path_manager.exists(path_manager.join_path(self.persist_directory, "index.faiss")):
                # æ—¢å­˜ã®FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰
                self.vectordb = FAISS.load_local(
                    self.persist_directory,
                    self.embedding_function,
                    allow_dangerous_deserialization=True
                )
                logger.info(f"Loaded FAISS index from {self.persist_directory}")
            else:
                # æ–°ã—ã„FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
                embedding_dim = 384  # ã‚ãªãŸã®ãƒ¢ãƒ‡ãƒ«æ¬¡ç¬¬ã§ç¢ºèª
                index = IndexFlatL2(embedding_dim)
                docstore = InMemoryDocstore()
                index_to_docstore_id = {}

                self.vectordb = FAISS(
                    index=index,
                    docstore=docstore,
                    index_to_docstore_id=index_to_docstore_id,
                    embedding_function=self.embedding_function,
                )
                logger.info("Created new FAISS index")
        except Exception as e:
            logger.error(f"Error setting up FAISS vector database: {e}", exc_info=True)
            raise VectorDBException(f"FAISSãƒ™ã‚¯ãƒˆãƒ«DBã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", details={
                "persist_directory": self.persist_directory,
                "error": str(e)
            })
    
    def _add_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’FAISSãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ ã™ã‚‹
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        if self.vectordb is None:
            raise VectorDBException("FAISSãƒ™ã‚¯ãƒˆãƒ«DBãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.vectordb.add_documents(documents)
    
    def _similarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        FAISSãƒ™ã‚¯ãƒˆãƒ«DBã§é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        if self.vectordb is None:
            raise VectorDBException("FAISSãƒ™ã‚¯ãƒˆãƒ«DBãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        return self.vectordb.similarity_search(query, k=k, filter=filter)
    
    def _similarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        FAISSãƒ™ã‚¯ãƒˆãƒ«DBã§ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        if self.vectordb is None:
            raise VectorDBException("FAISSãƒ™ã‚¯ãƒˆãƒ«DBãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        return self.vectordb.similarity_search_with_score(query, k=k, filter=filter)
    
    def _save(self) -> None:
        """FAISSãƒ™ã‚¯ãƒˆãƒ«DBã‚’ä¿å­˜ã™ã‚‹"""
        if self.vectordb is None:
            raise VectorDBException("FAISSãƒ™ã‚¯ãƒˆãƒ«DBãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if self.persist_directory:
            self.vectordb.save_local(self.persist_directory)
            logger.info(f"Saved FAISS index to {self.persist_directory}")
    
    async def _aadd_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’FAISSãƒ™ã‚¯ãƒˆãƒ«DBã«éåŒæœŸã§è¿½åŠ ã™ã‚‹
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        # FAISSã¯éåŒæœŸAPIã‚’æä¾›ã—ã¦ã„ãªã„ãŸã‚ã€åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        await asyncio.to_thread(self._add_documents, documents)
    
    async def _asimilarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        FAISSãƒ™ã‚¯ãƒˆãƒ«DBã§é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        # FAISSã¯éåŒæœŸAPIã‚’æä¾›ã—ã¦ã„ãªã„ãŸã‚ã€åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        return await asyncio.to_thread(self._similarity_search, query, k, filter)
    
    async def _asimilarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        FAISSãƒ™ã‚¯ãƒˆãƒ«DBã§ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        # FAISSã¯éåŒæœŸAPIã‚’æä¾›ã—ã¦ã„ãªã„ãŸã‚ã€åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        return await asyncio.to_thread(self._similarity_search_with_score, query, k, filter)
    
    async def _asave(self) -> None:
        """FAISSãƒ™ã‚¯ãƒˆãƒ«DBã‚’éåŒæœŸã§ä¿å­˜ã™ã‚‹"""
        # FAISSã¯éåŒæœŸAPIã‚’æä¾›ã—ã¦ã„ãªã„ãŸã‚ã€åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        await asyncio.to_thread(self._save)


class ChromaDBManager(VectorDBManager):
    """ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    
    def _setup_vectordb(self) -> None:
        """ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBã®è¨­å®š"""
        try:
            # ChromaDBã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’è¨­å®š
            from chromadb.config import Settings as ChromaSettings
            import chromadb
            
            # ChromaDBã®è¨­å®š
            chroma_settings = ChromaSettings(
                anonymized_telemetry=False,
                persist_directory=self.persist_directory
            )
            
            # ChromaDBã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
            client = chromadb.Client(chroma_settings)
            
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
            if self.persist_directory:
                # æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰
                self.vectordb = Chroma(
                    client=client,
                    collection_name=self.collection_name,
                    embedding_function=self.embedding_function,
                    persist_directory=self.persist_directory
                )
                logger.info(f"Loaded ChromaDB collection from {self.persist_directory}")
            else:
                # æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€æ–°ã—ã„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
                self.vectordb = Chroma(
                    client=client,
                    collection_name=self.collection_name,
                    embedding_function=self.embedding_function
                )
                logger.info(f"Created new ChromaDB collection: {self.collection_name}")
        except ImportError:
            logger.error("ChromaDB is not installed. Please install it with 'pip install chromadb'.")
            raise VectorDBException("ChromaDBãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'pip install chromadb'ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        except Exception as e:
            logger.error(f"Error setting up ChromaDB vector database: {e}", exc_info=True)
            raise VectorDBException(f"ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", details={
                "persist_directory": self.persist_directory,
                "collection_name": self.collection_name,
                "error": str(e)
            })
    
    def _add_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ ã™ã‚‹
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        if self.vectordb is None:
            raise VectorDBException("ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.vectordb.add_documents(documents)
    
    def _similarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBã§é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        if self.vectordb is None:
            raise VectorDBException("ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ChromaDBã®ãƒ•ã‚£ãƒ«ã‚¿å½¢å¼ã«å¤‰æ›
        where = filter if filter else None
        
        return self.vectordb.similarity_search(query, k=k, filter=where)
    
    def _similarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBã§ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        if self.vectordb is None:
            raise VectorDBException("ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ChromaDBã®ãƒ•ã‚£ãƒ«ã‚¿å½¢å¼ã«å¤‰æ›
        where = filter if filter else None
        
        return self.vectordb.similarity_search_with_score(query, k=k, filter=where)
    
    def _save(self) -> None:
        """ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBã‚’ä¿å­˜ã™ã‚‹"""
        if self.vectordb is None:
            raise VectorDBException("ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if self.persist_directory:
            self.vectordb.persist()
            logger.info(f"Saved ChromaDB collection to {self.persist_directory}")
    
    async def _aadd_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBã«éåŒæœŸã§è¿½åŠ ã™ã‚‹
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        # ChromaDBã¯éåŒæœŸAPIã‚’æä¾›ã—ã¦ã„ãªã„ãŸã‚ã€åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        await asyncio.to_thread(self._add_documents, documents)
    
    async def _asimilarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBã§é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        # ChromaDBã¯éåŒæœŸAPIã‚’æä¾›ã—ã¦ã„ãªã„ãŸã‚ã€åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        return await asyncio.to_thread(self._similarity_search, query, k, filter)
    
    async def _asimilarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBã§ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        # ChromaDBã¯éåŒæœŸAPIã‚’æä¾›ã—ã¦ã„ãªã„ãŸã‚ã€åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        return await asyncio.to_thread(self._similarity_search_with_score, query, k, filter)
    
    async def _asave(self) -> None:
        """ChromaDBãƒ™ã‚¯ãƒˆãƒ«DBã‚’éåŒæœŸã§ä¿å­˜ã™ã‚‹"""
        # ChromaDBã¯éåŒæœŸAPIã‚’æä¾›ã—ã¦ã„ãªã„ãŸã‚ã€åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        await asyncio.to_thread(self._save)


class VectorDBManagerFactory:
    """ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def create(
        db_type: str = "faiss",
        embedding_model: Optional[EmbeddingModel] = None,
        persist_directory: Optional[str] = None,
        collection_name: Optional[str] = None,
        **kwargs
    ) -> VectorDBManager:
        """
        ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆã™ã‚‹
        
        Args:
            db_type: ãƒ™ã‚¯ãƒˆãƒ«DBã®ç¨®é¡ ("faiss" ã¾ãŸã¯ "chroma")
            embedding_model: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
            persist_directory: æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        """
        if db_type.lower() == "faiss":
            return FAISSManager(
                embedding_model=embedding_model,
                persist_directory=persist_directory,
                collection_name=collection_name,
                **kwargs
            )
        elif db_type.lower() == "chroma":
            return ChromaDBManager(
                embedding_model=embedding_model,
                persist_directory=persist_directory,
                collection_name=collection_name,
                **kwargs
            )
        else:
            raise ValueError(f"Unsupported vector database type: {db_type}")
    
    @staticmethod
    def create_from_config(config: Dict[str, Any]) -> VectorDBManager:
        """
        è¨­å®šã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆã™ã‚‹
        
        Args:
            config: ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®è¨­å®š
            
        Returns:
            ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        """
        db_type = config.get("db_type", "faiss")
        persist_directory = config.get("persist_directory")
        collection_name = config.get("collection_name")
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
        embedding_config = config.get("embedding", {})
        embedding_model = None
        if embedding_config:
            embedding_model = EmbeddingModelFactory.create_from_config(embedding_config)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š
        cache_config = config.get("cache", {})
        
        # ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        kwargs = {k: v for k, v in config.items() if k not in ["db_type", "persist_directory", "collection_name", "embedding", "cache"]}
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šã‚’è¿½åŠ 
        kwargs["cache_config"] = cache_config
        
        return VectorDBManagerFactory.create(
            db_type=db_type,
            embedding_model=embedding_model,
            persist_directory=persist_directory,
            collection_name=collection_name,
            **kwargs
        )
    
    @staticmethod
    def create_default(service_id: Optional[str] = None) -> VectorDBManager:
        """
        ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆã™ã‚‹
        
        Args:
            service_id: ã‚µãƒ¼ãƒ“ã‚¹ID
            
        Returns:
            ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        """
        db_type = os.environ.get("VECTOR_DB_TYPE", "faiss")
        logger.info(f"ğŸ“Œ VECTOR_DB_TYPE = {db_type}")
        
        data_dir = os.environ.get("DATA_DIR", "/app/data")
        persist_directory = path_manager.join_path(data_dir, db_type, service_id) if service_id else None
        logger.info(f"ğŸ“‚ persist_directory = {persist_directory}")
        
        collection_name = service_id if service_id else "default"
        logger.info(f"ğŸ“ collection_name = {collection_name}")

        logger.info("ğŸ§  Creating embedding model...")
        embedding_model = EmbeddingModelFactory.create_default()
        logger.info("âœ… Embedding model created")

        logger.info("ğŸ§± Creating vector DB manager...")
        return VectorDBManagerFactory.create(
            db_type=db_type,
            embedding_model=embedding_model,
            persist_directory=persist_directory,
            collection_name=collection_name
        )
