"""
ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ç•°ãªã‚‹ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å¯¾ã—ã¦çµ±ä¸€çš„ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚
åŒæœŸå‡¦ç†ã¨éåŒæœŸå‡¦ç†ã®ä¸¡æ–¹ã«å¯¾å¿œã—ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ã¨ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ã‚’çµ„ã¿è¾¼ã‚“ã§ã„ã¾ã™ã€‚
"""

import abc
import os
import json
import pickle
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, TypeVar, Tuple
import hashlib

from app.config import settings
from app.exceptions import CaseforgeException, ErrorCode
from app.utils.path_manager import path_manager
from app.logging_config import logger
from app.utils.retry import retry, async_retry, RetryStrategy
from app.utils.timeout import timeout, async_timeout

from langchain_core.documents import Document
from langchain_community.vectorstores import PGVector
from langchain_community.docstore.in_memory import InMemoryDocstore

from app.services.vector_db.embeddings import (
    EmbeddingModel,
    EmbeddingModelFactory,
    EmbeddingModelWrapper,
    EmbeddingException
)

T = TypeVar('T')
VectorDBManagerType = TypeVar('VectorDBManagerType', bound='VectorDBManager')


class VectorDBException(CaseforgeException):
    """ãƒ™ã‚¯ãƒˆãƒ«DBé–¢é€£ã®ä¾‹å¤–"""
    def __init__(
        self,
        message: str = "ãƒ™ã‚¯ãƒˆãƒ«DBæ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
        details: Optional[Dict[str, Any]] = None
    ):
        super().__init__(message, ErrorCode.VECTOR_DB_ERROR, details)


class DocumentCache:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, cache_dir: str = None, ttl: int = 3600):
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åˆæœŸåŒ–
        
        Args:
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            ttl: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æœŸé–“ï¼ˆç§’ï¼‰
        """
        self.cache_dir = cache_dir or path_manager.join_path(
            os.environ.get("DATA_DIR", "/app/data"),
            "document_cache"
        )
        self.ttl = ttl
        
        path_manager.ensure_dir(self.cache_dir)
    
    def _get_cache_key(self, key: str) -> str:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’å–å¾—
        
        Args:
            key: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
            
        Returns:
            ãƒãƒƒã‚·ãƒ¥åŒ–ã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        """
        return hashlib.md5(key.encode()).hexdigest()
    
    def _get_cache_path(self, key: str) -> str:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
        
        Args:
            key: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
            
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        cache_key = self._get_cache_key(key)
        return path_manager.join_path(self.cache_dir, f"{cache_key}.pkl")
    
    def get(self, key: str) -> Optional[List[Document]]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        
        Args:
            key: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
            
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€å­˜åœ¨ã—ãªã„å ´åˆã¯None
        """
        cache_path = self._get_cache_path(key)
        
        if not path_manager.exists(cache_path):
            return None
        
        mtime = os.path.getmtime(str(cache_path))
        if datetime.fromtimestamp(mtime) + timedelta(seconds=self.ttl) < datetime.now():
            os.remove(cache_path)
            return None
        
        try:
            with open(cache_path, "rb") as f:
                return pickle.load(f)
        except Exception as e:
            logger.error(f"Error loading document cache: {e}", exc_info=True)
            os.remove(cache_path)
            return None
    
    def set(self, key: str, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        
        Args:
            key: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
            documents: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        cache_path = self._get_cache_path(key)
        
        try:
            with open(cache_path, "wb") as f:
                pickle.dump(documents, f)
        except Exception as e:
            logger.error(f"Error saving document cache: {e}", exc_info=True)
            if path_manager.exists(cache_path):
                os.remove(str(cache_path))
    
    def clear(self, key: Optional[str] = None) -> None:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        
        Args:
            key: ã‚¯ãƒªã‚¢ã™ã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã€Noneã®å ´åˆã¯å…¨ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        """
        if key is None:
            for file in os.listdir(str(self.cache_dir)):
                if file.endswith(".pkl"):
                    os.remove(path_manager.join_path(self.cache_dir, file))
        else:
            cache_path = self._get_cache_path(key)
            if path_manager.exists(cache_path):
                os.remove(str(cache_path))
    
    def cleanup_expired(self) -> int:
        """
        æœŸé™åˆ‡ã‚Œã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        
        Returns:
            å‰Šé™¤ã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ•°
        """
        count = 0


class VectorDBManager(abc.ABC):
    """ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        embedding_model: Optional[EmbeddingModel] = None,
        persist_directory: Optional[str] = None,
        collection_name: Optional[str] = None,
        timeout_seconds: Optional[float] = None,
        retry_config: Optional[Dict[str, Any]] = None,
        cache_config: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        """
        ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
        
        Args:
            embedding_model: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
            persist_directory: æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
            timeout_seconds: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
            retry_config: ãƒªãƒˆãƒ©ã‚¤è¨­å®š
            cache_config: ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        self.embedding_model = embedding_model or EmbeddingModelFactory.create_default()
        self.embedding_function = EmbeddingModelWrapper(self.embedding_model)
        
        self.persist_directory = persist_directory
        if self.persist_directory:
            os.makedirs(self.persist_directory, exist_ok=True)
        
        self.collection_name = collection_name or "default"
        
        self.timeout_seconds = timeout_seconds or settings.TIMEOUT_EMBEDDING
        self.retry_config = retry_config or {
            "max_retries": 3,
            "retry_delay": 2.0,
            "max_retry_delay": 10.0,
            "retry_jitter": 0.1,
            "backoff_factor": 2.0,
            "retry_strategy": RetryStrategy.EXPONENTIAL,
            "retry_exceptions": [ConnectionError, TimeoutError, ValueError]
        }
        
        cache_config = cache_config or {}
        self.document_cache = DocumentCache(
            cache_dir=cache_config.get("cache_dir"),
            ttl=cache_config.get("ttl", 3600)
        )
        self.use_cache = cache_config.get("use_cache", True)
        self.extra_params = kwargs
        self.vectordb = None
        self._setup_vectordb()
    
    @abc.abstractmethod
    def _setup_vectordb(self) -> None:
        """ãƒ™ã‚¯ãƒˆãƒ«DBã®è¨­å®šï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        pass
    
    @abc.abstractmethod
    def _add_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ ã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    def _similarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    def _similarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    def _save(self) -> None:
        """
        ãƒ™ã‚¯ãƒˆãƒ«DBã‚’ä¿å­˜ã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        """
        pass
    
    @abc.abstractmethod
    async def _aadd_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«éåŒæœŸã§è¿½åŠ ã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    async def _asimilarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    async def _asimilarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        pass
    
    @abc.abstractmethod
    async def _asave(self) -> None:
        """
        ãƒ™ã‚¯ãƒˆãƒ«DBã‚’éåŒæœŸã§ä¿å­˜ã™ã‚‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰
        """
        pass
    
    def _get_cache_key_for_documents(self, documents: List[Document]) -> str:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            documents: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        """
        content_hash = hashlib.md5()
        for doc in documents:
            content_hash.update(doc.page_content.encode())
            if doc.metadata:
                content_hash.update(json.dumps(doc.metadata, sort_keys=True).encode())
        
        return f"docs_{content_hash.hexdigest()}"
    
    def _get_cache_key_for_query(self, query: str, k: int, filter: Optional[Dict[str, Any]]) -> str:
        """
        ã‚¯ã‚¨ãƒªã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        """
        query_hash = hashlib.md5()
        query_hash.update(query.encode())
        query_hash.update(str(k).encode())
        if filter:
            query_hash.update(json.dumps(filter, sort_keys=True).encode())
        
        return f"query_{query_hash.hexdigest()}"
    
    @retry(retry_key="VECTOR_DB")
    @timeout(timeout_key="VECTOR_DB")
    def add_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ ã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        try:
            logger.info(f"Adding {len(documents)} documents to vector database")
            
            if self.use_cache:
                self.document_cache.clear()
            
            self._add_documents(documents)
            
            if self.persist_directory:
                self._save()
            
            logger.info(f"Successfully added {len(documents)} documents to vector database")
        except Exception as e:
            logger.error(f"Error adding documents to vector database: {e}", exc_info=True)
            raise VectorDBException(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "error": str(e)
            })
    
    @retry(retry_key="VECTOR_DB")
    @timeout(timeout_key="VECTOR_DB")
    def similarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        try:
            logger.info(f"Performing similarity search for query: {query[:30]}...")
            
            if self.use_cache:
                cache_key = self._get_cache_key_for_query(query, k, filter)
                cached_results = self.document_cache.get(cache_key)
                if cached_results:
                    logger.info(f"Using cached results for query: {query[:30]}...")
                    return cached_results
            
            results = self._similarity_search(query, k, filter)
            
            if self.use_cache:
                self.document_cache.set(cache_key, results)
            
            logger.info(f"Successfully performed similarity search, found {len(results)} documents")
            return results
        except Exception as e:
            logger.error(f"Error performing similarity search: {e}", exc_info=True)
            raise VectorDBException(f"é¡ä¼¼åº¦æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "query": query,
                "error": str(e)
            })
    
    @retry(retry_key="VECTOR_DB")
    @timeout(timeout_key="VECTOR_DB")
    def similarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        try:
            logger.info(f"Performing similarity search with score for query: {query[:30]}...")            
            results = self._similarity_search_with_score(query, k, filter)
            
            logger.info(f"Successfully performed similarity search with score, found {len(results)} documents")
            return results
        except Exception as e:
            logger.error(f"Error performing similarity search with score: {e}", exc_info=True)
            raise VectorDBException(f"ã‚¹ã‚³ã‚¢ä»˜ãé¡ä¼¼åº¦æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "query": query,
                "error": str(e)
            })
    
    @async_retry(retry_key="VECTOR_DB")
    @async_timeout(timeout_key="VECTOR_DB")
    async def aadd_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«éåŒæœŸã§è¿½åŠ ã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        try:
            logger.info(f"Async adding {len(documents)} documents to vector database")
            if self.use_cache:
                self.document_cache.clear()
            
            await self._aadd_documents(documents)
            
            if self.persist_directory:
                await self._asave()
            
            logger.info(f"Successfully async added {len(documents)} documents to vector database")
        except Exception as e:
            logger.error(f"Error async adding documents to vector database: {e}", exc_info=True)
            raise VectorDBException(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆéåŒæœŸè¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "error": str(e)
            })
    
    @async_retry(retry_key="VECTOR_DB")
    @async_timeout(timeout_key="VECTOR_DB")
    async def asimilarity_search(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        try:
            logger.info(f"Async performing similarity search for query: {query[:30]}...")
            if self.use_cache:
                cache_key = self._get_cache_key_for_query(query, k, filter)
                cached_results = self.document_cache.get(cache_key)
                if cached_results:
                    logger.info(f"Using cached results for query: {query[:30]}...")
                    return cached_results
            
            results = await self._asimilarity_search(query, k, filter)
            
            if self.use_cache:
                self.document_cache.set(cache_key, results)
            
            logger.info(f"Successfully async performed similarity search, found {len(results)} documents")
            return results
        except Exception as e:
            logger.error(f"Error async performing similarity search: {e}", exc_info=True)
            raise VectorDBException(f"éåŒæœŸé¡ä¼¼åº¦æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "query": query,
                "error": str(e)
            })
    
    @async_retry(retry_key="VECTOR_DB")
    @async_timeout(timeout_key="VECTOR_DB")
    async def asimilarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        try:
            logger.info(f"Async performing similarity search with score for query: {query[:30]}...")            
            results = await self._asimilarity_search_with_score(query, k, filter)
            
            logger.info(f"Successfully async performed similarity search with score, found {len(results)} documents")
            return results
        except Exception as e:
            logger.error(f"Error async performing similarity search with score: {e}", exc_info=True)
            raise VectorDBException(f"éåŒæœŸã‚¹ã‚³ã‚¢ä»˜ãé¡ä¼¼åº¦æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "query": query,
                "error": str(e)
            })
    
    def clear_cache(self) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        if self.use_cache:
            self.document_cache.clear()
            logger.info("Document cache cleared")
        for file in os.listdir(str(self.cache_dir)):
            if file.endswith(".pkl"):
                file_path = path_manager.join_path(self.cache_dir, file)
                mtime = os.path.getmtime(str(file_path))
                if datetime.fromtimestamp(mtime) + timedelta(seconds=self.ttl) < datetime.now():
                    os.remove(str(file_path))
                    count += 1
        return count


class VectorDBManagerFactory:
    """ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def create(
        db_type: str = "pgvector",
        embedding_model: Optional[EmbeddingModel] = None,
        persist_directory: Optional[str] = None,
        collection_name: Optional[str] = None,
        **kwargs
    ) -> VectorDBManager:
        """
        ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆã™ã‚‹
        
        Args:
            db_type: ãƒ™ã‚¯ãƒˆãƒ«DBã®ç¨®é¡ ("pgvector")
            embedding_model: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
            persist_directory: æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        """
        if db_type.lower() == "pgvector":
            return PGVectorManager(
                embedding_model=embedding_model,
                collection_name=collection_name,
                **kwargs
            )
        else:
            raise ValueError(f"Unsupported vector database type: {db_type}")
    
    @staticmethod
    def create_from_config(config: Dict[str, Any]) -> VectorDBManager:
        """
        è¨­å®šã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆã™ã‚‹
        
        Args:
            config: ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®è¨­å®š
            
        Returns:
            ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        """
        db_type = config.get("db_type", "pgvector")
        persist_directory = config.get("persist_directory")
        collection_name = config.get("collection_name")
        
        embedding_config = config.get("embedding", {})
        embedding_model = None
        if embedding_config:
            embedding_model = EmbeddingModelFactory.create_from_config(embedding_config)
        
        cache_config = config.get("cache", {})
        
        kwargs = {k: v for k, v in config.items() if k not in ["db_type", "persist_directory", "collection_name", "embedding", "cache", "service_id"]}
        
        kwargs["cache_config"] = cache_config
        
        service_id = config.get("service_id")
        
        return VectorDBManagerFactory.create(
            db_type=db_type,
            embedding_model=embedding_model,
            persist_directory=persist_directory,
            collection_name=collection_name,
            service_id=service_id,
            **kwargs
        )
    
    @staticmethod
    def create_default(service_id: Optional[int] = None) -> VectorDBManager:
        """
        ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆã™ã‚‹
        
        Args:
            service_id: ã‚µãƒ¼ãƒ“ã‚¹ID
            
        Returns:
            ãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        """
        db_type = os.environ.get("VECTOR_DB_TYPE", "pgvector")
        logger.info(f"ğŸ“Œ VECTOR_DB_TYPE = {db_type}")
        
        data_dir = os.environ.get("DATA_DIR", "/app/data")
        persist_directory = path_manager.join_path(data_dir, db_type, service_id) if service_id else None
        logger.info(f"ğŸ“‚ persist_directory = {persist_directory}")
        
        collection_name = str(service_id) if service_id is not None else "default"
        logger.info(f"ğŸ“ collection_name = {collection_name}")

        logger.info("ğŸ§  Creating embedding model...")
        embedding_model = EmbeddingModelFactory.create_default()
        logger.info("âœ… Embedding model created")

        logger.info("ğŸ§± Creating vector DB manager...")
        
        if db_type == "pgvector":
            return PGVectorManager(
                embedding_model=embedding_model,
                collection_name=collection_name,
                timeout_seconds=settings.TIMEOUT_EMBEDDING,
                retry_config=None,
                cache_config=None,
                service_id=service_id
            )
        
        if db_type.lower() == "pgvector":
            return PGVectorManager(
                embedding_model=embedding_model,
                collection_name=collection_name,
                timeout_seconds=settings.TIMEOUT_EMBEDDING,
                retry_config=None,
                cache_config=None,
                service_id=service_id
            )
        else:
            raise VectorDBException(f"Unsupported vector database type: {db_type}")
    
import os
from typing import List, Dict, Any, Optional, Tuple

from sqlmodel import SQLModel, Field, create_engine, Session, select
from pgvector.sqlalchemy import Vector
from sqlalchemy import Column

from langchain_core.documents import Document

from app.config import settings
from app.models.schema_chunk import SchemaChunk
from app.services.vector_db.embeddings import EmbeddingModelWrapper
from app.logging_config import logger

DATABASE_URL = settings.DATABASE_URL

class PGVectorManager(VectorDBManager):
    """PGVectorãƒ™ã‚¯ãƒˆãƒ«DBãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(
        self,
        embedding_model: Optional[EmbeddingModelWrapper] = None,
        collection_name: Optional[str] = None,
        timeout_seconds: Optional[float] = None,
        retry_config: Optional[Dict[str, Any]] = None,
        cache_config: Optional[Dict[str, Any]] = None,
        service_id: Optional[int] = None,
        **kwargs
    ):
        """
        PGVectorãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–

        Args:
            embedding_model: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼
            collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åï¼ˆã“ã“ã§ã¯service_idã‚’ä½¿ç”¨ï¼‰
            timeout_seconds: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
            retry_config: ãƒªãƒˆãƒ©ã‚¤è¨­å®š
            cache_config: ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
            service_id: ã‚µãƒ¼ãƒ“ã‚¹ID (å¿…é ˆ)
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        if service_id is None:
            raise ValueError("service_id must be provided for PGVectorManager")

        self.service_id = service_id
        # collection_name ã¯ service_id ã‚’ä½¿ç”¨ã™ã‚‹
        collection_name = str(service_id)

        super().__init__(
            embedding_model=embedding_model,
            collection_name=collection_name,
            timeout_seconds=timeout_seconds,
            retry_config=retry_config,
            cache_config=cache_config,
            **kwargs
        )

        self.engine = create_engine(DATABASE_URL)
        SQLModel.metadata.create_all(self.engine) # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ

    def _setup_vectordb(self) -> None:
        """PGVectorã®è¨­å®š"""
        # PGVectorã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è‡ªä½“ãŒãƒ™ã‚¯ãƒˆãƒ«DBã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ãŸã‚ã€ç‰¹åˆ¥ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯ä¸è¦
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®å­˜åœ¨ç¢ºèªã‚„ä½œæˆã¯__init__ã§è¡Œã†
        # ä¾‹: CREATE INDEX ON schema_chunk USING ivfflat(embedding vector_l2_ops) WITH (lists = 100);
        logger.info(f"PGVectorManager setup complete for service_id: {self.service_id}")
        pass

    def _add_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’PGVectorã«è¿½åŠ ã™ã‚‹

        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        logger.info(f"Adding {len(documents)} documents to PGVector for service_id: {self.service_id}")
        schema_chunks = []
        for doc in documents:
            # Documentã®metadataã‹ã‚‰pathã¨methodã‚’å–å¾—ã™ã‚‹ã“ã¨ã‚’æƒ³å®š
            path = doc.metadata.get("path")
            method = doc.metadata.get("method")
            if not path or not method:
                logger.warning(f"Skipping document due to missing path or method in metadata: {doc.metadata}")
                continue

            try:
                # embedding_functionã¯VectorDBManagerã®__init__ã§åˆæœŸåŒ–æ¸ˆã¿
                embedding = self.embedding_function.embed_query(doc.page_content)
                schema_chunk = SchemaChunk(
                    service_id=self.service_id,
                    path=path,
                    method=method,
                    content=doc.page_content,
                    embedding=embedding
                )
                schema_chunks.append(schema_chunk)
            except Exception as e:
                logger.error(f"Error creating SchemaChunk for document: {doc.metadata}. Error: {e}", exc_info=True)
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã€å‡¦ç†ã‚’ç¶šè¡Œ

        if not schema_chunks:
            logger.warning("No valid schema chunks to add.")
            return

        with Session(self.engine) as session:
            try:
                session.add_all(schema_chunks)
                session.commit()
                logger.info(f"Successfully added {len(schema_chunks)} schema chunks to PGVector.")
            except Exception as e:
                session.rollback()
                logger.error(f"Error adding schema chunks to database: {e}", exc_info=True)
                raise VectorDBException(f"ã‚¹ã‚­ãƒ¼ãƒãƒãƒ£ãƒ³ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                    "service_id": self.service_id,
                    "error": str(e)
                })

    def _similarity_search(
        self,
        query: str,
        k: int = 4,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        PGVectorã§é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ (service_idã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æƒ³å®š)

        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        logger.info(f"Performing PGVector similarity search for query: {query[:30]}... with k={k} and filter={filter}")
        try:
            # ã‚¯ã‚¨ãƒªã®embeddingã‚’ç”Ÿæˆ
            query_embedding = self.embedding_function.embed_query(query)

            with Session(self.engine) as session:
                # é¡ä¼¼åº¦æ¤œç´¢ã‚¯ã‚¨ãƒªã®æ§‹ç¯‰
                # service_id ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã€embedding ã®é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
                # é¡ä¼¼åº¦æ¼”ç®—å­ '<->' ã¯L2è·é›¢ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ï¼‰
                # è·é›¢ãŒå°ã•ã„ã»ã©é¡ä¼¼åº¦ãŒé«˜ã„ã®ã§ã€æ˜‡é †ã§ã‚½ãƒ¼ãƒˆ
                statement = select(SchemaChunk).where(
                    SchemaChunk.service_id == self.service_id
                ).order_by(
                    SchemaChunk.embedding.l2_distance(query_embedding)
                ).limit(k)

                results = session.exec(statement).all()

                # çµæœã‚’LangChainã®Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                documents = []
                for chunk in results:
                    metadata = {
                        "service_id": chunk.service_id,
                        "path": chunk.path,
                        "method": chunk.method,
                        # embedding ã¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å«ã‚ãªã„
                    }
                    documents.append(Document(page_content=chunk.content, metadata=metadata))

                logger.info(f"PGVector similarity search found {len(documents)} documents.")
                return documents

        except Exception as e:
            logger.error(f"Error performing PGVector similarity search: {e}", exc_info=True)
            raise VectorDBException(f"PGVectoré¡ä¼¼åº¦æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "query": query,
                "k": k,
                "filter": filter,
                "error": str(e)
            })

    def _similarity_search_with_score(
        self,
        query: str,
        k: int = 4,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        PGVectorã§ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ (service_idã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æƒ³å®š)

        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        logger.info(f"Performing PGVector similarity search with score for query: {query[:30]}... with k={k} and filter={filter}")
        try:
            # ã‚¯ã‚¨ãƒªã®embeddingã‚’ç”Ÿæˆ
            query_embedding = self.embedding_function.embed_query(query)

            with Session(self.engine) as session:
                # é¡ä¼¼åº¦æ¤œç´¢ã‚¯ã‚¨ãƒªã®æ§‹ç¯‰ï¼ˆã‚¹ã‚³ã‚¢ä»˜ãï¼‰
                # ã‚¹ã‚³ã‚¢ã¯L2è·é›¢ã®é€†æ•°ã‚„ã€1 - è·é›¢/æœ€å¤§è·é›¢ãªã©ã§æ­£è¦åŒ–ã™ã‚‹ã“ã¨ã‚‚è€ƒãˆã‚‰ã‚Œã‚‹ãŒã€
                # ã“ã“ã§ã¯L2è·é›¢ãã®ã‚‚ã®ã‚’ã‚¹ã‚³ã‚¢ã¨ã—ã¦è¿”ã™ï¼ˆè·é›¢ãŒå°ã•ã„ã»ã©é¡ä¼¼åº¦ãŒé«˜ã„ï¼‰
                statement = select(SchemaChunk, SchemaChunk.embedding.l2_distance(query_embedding)).where(
                     SchemaChunk.service_id == self.service_id
                ).order_by(
                    SchemaChunk.embedding.l2_distance(query_embedding)
                ).limit(k)

                results = session.exec(statement).all()

                # çµæœã‚’LangChainã®Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã«å¤‰æ›
                documents_with_score = []
                for chunk, score in results:
                    metadata = {
                        "service_id": chunk.service_id,
                        "path": chunk.path,
                        "method": chunk.method,
                    }
                    documents_with_score.append((Document(page_content=chunk.content, metadata=metadata), score))

                logger.info(f"PGVector similarity search with score found {len(documents_with_score)} documents.")
                return documents_with_score

        except Exception as e:
            logger.error(f"Error performing PGVector similarity search with score: {e}", exc_info=True)
            raise VectorDBException(f"PGVectorã‚¹ã‚³ã‚¢ä»˜ãé¡ä¼¼åº¦æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", details={
                "query": query,
                "k": k,
                "filter": filter,
                "error": str(e)
            })


    def _save(self) -> None:
        """
        PGVectorã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ°¸ç¶šåŒ–ã•ã‚Œã‚‹ãŸã‚ã€ç‰¹åˆ¥ãªä¿å­˜å‡¦ç†ã¯ä¸è¦
        """
        logger.info("PGVector does not require explicit save operation.")
        pass

    async def _asave(self) -> None:
        """
        PGVectorã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«éåŒæœŸã§æ°¸ç¶šåŒ–ã•ã‚Œã‚‹ãŸã‚ã€ç‰¹åˆ¥ãªä¿å­˜å‡¦ç†ã¯ä¸è¦
        """
        logger.info("PGVector does not require explicit asynchronous save operation.")
        pass

    async def _aadd_documents(self, documents: List[Document]) -> None:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’PGVectorã«éåŒæœŸã§è¿½åŠ ã™ã‚‹

        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        logger.warning("Asynchronous add_documents not fully implemented for PGVectorManager, falling back to sync.")
        self._add_documents(documents)

    async def _asimilarity_search(
        self,
        query: str,
        k: int = 4,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        PGVectorã§é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿

        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        logger.warning("Asynchronous similarity_search not fully implemented for PGVectorManager, falling back to sync.")
        return self._similarity_search(query, k, filter)

    async def _asimilarity_search_with_score(
        self,
        query: str,
        k: int = 4,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[Document, float]]:
        """
        PGVectorã§ã‚¹ã‚³ã‚¢ä»˜ãã®é¡ä¼¼åº¦æ¤œç´¢ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            filter: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿

        Returns:
            é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚¹ã‚³ã‚¢ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        logger.warning("Asynchronous similarity_search_with_score not fully implemented for PGVectorManager, falling back to sync.")
        return self._similarity_search_with_score(query, k, filter)
